# -*- coding: utf-8 -*-
"""MoizLLM(Gemini).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1se5FosvrjefXbC9Y3s0v3aVl8I6E5A5E
"""

import os
import gdown

# Download the file from Google Drive using its shareable link
gdown.download(
    "https://drive.google.com/drive/folders/1C6ImLPbM7i17vKZ4iqUw3SMjOz9_mhHC?usp=sharing",
    "moiz",
    fuzzy=True,
    quiet=False
)

import pandas as pd
import os
import csv
import json
import re
import time
import google.generativeai as genai

from google.colab import files
# from google.colab import files

# Configure Google GenAI with API key

api_key = os.getenv("GEMINI_API_KEY")  # Replace with your API key
genai.configure(api_key=api_key)

# Define the model name (ensure it's the correct one for your case)
model_name = "gemini-1.5-flash"  # Adjust this based on your model's ID
def calculate_tokens(prompt, model_name=model_name):
    # Load the tokenizer using 'cl100k_base' encoding if model_name is "gemini-1.5-flash"
    if model_name == "gemini-1.5-flash":
        encoding_name = "cl100k_base"
    else:
        encoding_name = tiktoken.encoding_name_for_model(model_name)
    tokenizer = tiktoken.get_encoding(encoding_name)  # Use get_encoding to explicitly get the tokenizer
    tokens = tokenizer.encode(prompt)
    return len(tokens)

# Example usage

# Load the dataset
file_path = '/content/drive/My Drive/Output_Folders/moiz/split_pak_file_3.csv'

for i in range(3, 3 * 17, 3):  # Increment by 3 (3, 6, 9, ..., 48 for 16 files)
    file_name = f"split_pak_file_{i}.csv"
    file_path = os.path.join(folder_path, file_name)
    
    if os.path.exists(file_path):
        print(f"Processing file: {file_name}")
        
        # Read the file into a DataFrame
        df = pd.read_csv(file_path)
        
        # Display a sample of the dataset (for debugging purposes)
        print(df.head())

# Define batch and sub-batch sizes
sub_batch_size = 2  # Smaller sub-batches to avoid token limit issues

# Define the base prompt
base_prompt = """
Provide the following JSON for each unique job role. Ensure no duplicate rows in the output. Each row should represent a distinct job role, identified by a unique combination of attributes like "Job Title", "Company", "Date Posted".

[
    {
        "Job Title": "",
        "Company": "",
        "Location": "",
        "Date Posted": "",
        "Salary": "",
        "Technical skills": "",
        "Industry": "",
        "Experience required": "",
        "Soft skills": "",
        "Education needed": "",
        "Personality traits": {
            "MBTI": "Determine the MBTI type based on the job title and job description. Use the dominant cognitive functions and behavioral patterns that align with the role. Format: 'MBTI Type', e.g., 'INTJ' for a strategic planner role.",
            "RIASEC": "Comma-separated dominant traits (e.g., 'Investigative, Conventional'). Derive from job responsibilities and nature of work.",
            "Big Five": {
                "Conscientiousness": "Rate on a scale of 1-5 based on job demands (e.g., highly structured roles like 'Accountant' would be rated 5, while creative roles like 'Graphic Designer' may be rated lower).",
                "Openness": "Assess based on the role's need for creativity and adaptability. High for research and creative roles, lower for process-driven jobs.",
                "Extraversion": "Rate high for customer-facing or leadership roles, lower for solitary analytical work.",
                "Agreeableness": "High for roles requiring teamwork, empathy, or customer service; lower for competitive, high-pressure environments.",
                "Neuroticism": "Consider job stress levels; fast-paced, high-risk roles may have higher neuroticism scores."
            }
        }
    }
]

Guidelines:
1. Use double quotes for all keys/values to ensure valid JSON.
2. *MBTI Assessment:* Identify the most relevant MBTI type by analyzing the job title and description. Consider cognitive functions, decision-making style, and interaction patterns. For example:
   - A "Data Scientist" might be *INTP (Logician)* due to analytical and problem-solving skills.
   - A "Marketing Manager" might be *ENTP (Debater)* for strategic thinking and persuasion.
3. *Big Five Traits:* Evaluate based on job description:
   - *Conscientiousness* → Higher for structured, detail-oriented roles (e.g., Accountants, Project Managers).
   - *Openness* → Higher for creative, exploratory roles (e.g., Designers, Researchers).
   - *Extraversion* → Higher for leadership and public-facing roles (e.g., Sales, Management).
   - *Agreeableness* → Higher for teamwork-oriented roles (e.g., HR, Social Work).
   - *Neuroticism* → Higher for stressful, high-stakes roles (e.g., Emergency Responders).
4. *Industry Classification:* Assign the correct *"Industry"* based on the *nature of the job itself, using standardized categories like **"Information Technology," "Healthcare," "Finance," "Legal Services,"* etc. Do *not* determine the industry based on the company's domain—focus on the actual job role and responsibilities.
   - An *"Office Boy"* should fall under *"Administrative & Support Services,"* regardless of whether they work in a tech company, hospital, or bank.
   - A *"Legal Advisor"* should be categorized under *"Legal Services,"* even if they work at a financial institution.
5. *RIASEC Traits:* Assign based on the nature of work. For example:
   - *Software Engineers* → Investigative, Conventional
   - *Graphic Designers* → Artistic, Investigative
   - *Teachers* → Social, Enterprising
6. *Experience Formatting:* Standardize experience requirements using *only numbers and "months"/"years"*, following these rules:
   - If a job states a *specific number* (e.g., "7+ years required"), use *only the number and unit* → "7 years".
   - If experience is *preferred*, set it to → "0 years".
   - If no experience requirement is mentioned, set it to → "0 years".
   - If experience is mentioned but *not quantified* (e.g., "Hands-on experience required"), assume a default → "6 months".
7. *Education Formatting:* Standardize degree requirements using these rules:
   - If a specific field is mentioned → "Bachelors in Computer Science", "Masters in Psychology", etc.
   - If only degree level is mentioned → "Bachelors Any", "Masters Any".
   - For lower levels → "Matric", "Inter".
   - If no education requirement is provided → "N/A".
8. Extract "Technical skills" and "Soft skills" directly from the job description, ensuring alignment with the role’s responsibilities.
9. Standardize job titles where possible (e.g., "Software Engineer" and "Software Developer" should have a unified classification).
10. If any information (e.g., salary, education required) is missing or not specified, mark it as "N/A."
11. Ensure the final output is a valid JSON array with no syntax errors.
"""

# Retry and rate limit configurations
max_retries = 5
rate_limit_delay = 5  # Seconds between API calls

# Prepare output files
output_file = "output1.csv"
skipped_file = "skipped_batches.txt"
error_log_file = "error_logs.txt"
output_folder = "/content/drive/My Drive/LLM Response/Moiz"
existing_file_path = os.path.join(output_folder, "output1.csv")


# Initialize files
for file in [output_file, skipped_file, error_log_file]:
    with open(file, mode="w", encoding="utf-8") as f:
        f.write("")  # Clear content

# Process dataset in sub-batches
for sub_batch_start in range(0, len(df), sub_batch_size):
    sub_batch = df.iloc[sub_batch_start:sub_batch_start + sub_batch_size]

    # Convert the sub-batch to a string for input
    data_excerpt = sub_batch.to_string(index=False)

    # Combine prompt and sub-batch to form the full input
    prompt = f"Here is a dataset:\n{data_excerpt}\n{base_prompt}"
    print(f"Processing sub-batch starting at index {sub_batch_start}")
    # num_tokens = calculate_tokens(prompt, model_name="gemini-1.5-flash")
    # print(f"Sub-batch {sub_batch_start} prompt contains {num_tokens} tokens.")


    # Initialize the Generative Model
    model = genai.GenerativeModel(model_name)

    # Retry mechanism for API requests
    retry_count = 0
    response_content = None
    while retry_count < max_retries:
        try:
            # API call with Google GenAI using the configured API key
            response = model.generate_content(prompt)
            response_content = response.text.strip()  # Assuming 'text' contains the generated content
            time.sleep(rate_limit_delay)  # Add delay to handle rate limits
            break  # Exit retry loop on success
        except Exception as e:
            retry_count += 1
            wait_time = rate_limit_delay # Exponential backoff with cap
            print(f"Request failed: {e}. Retrying in {wait_time} seconds...")
            time.sleep(wait_time)

    if response_content is None:
        print(f"Max retries reached for sub-batch {sub_batch_start}. Skipping.")
        with open(skipped_file, mode="a") as log_file:
            log_file.write(f"Skipped sub-batch {sub_batch_start}:\n{data_excerpt}\n")
        continue

    # Extract and process the response
    try:
        response_content = re.search(r"\[.*?\]", response_content, re.DOTALL).group(0)
        data = json.loads(response_content)
    except Exception as e:
        print(f"Error decoding JSON for sub-batch {sub_batch_start}: {e}")
        with open(error_log_file, mode="a") as log_file:
            log_file.write(f"Error for sub-batch {sub_batch_start}:\n{response_content}\n")
        continue

    # Write the extracted data to CSV
    if isinstance(data, list) and all(isinstance(item, dict) for item in data):
        with open(output_file, mode="a", newline="", encoding="utf-8") as csvfile:
            writer = csv.writer(csvfile)
            if csvfile.tell() == 0:  # If file is empty, write headers
                writer.writerow(data[0].keys())
            for item in data:
                writer.writerow(item.values())
        print(f"Sub-batch data successfully written to {output_file}.")
    else:
        print(f"The response for sub-batch {sub_batch_start} is not in the expected JSON list format.")
        with open(error_log_file, mode="a") as log_file:
            log_file.write(f"Invalid format for sub-batch {sub_batch_start}:\n{response_content}\n")

# Display the file content
import os
from google.colab import files
output1 = output_file
# Define the output folder
output_folder = "/content/drive/My Drive/LLM Response/Moiz"
# Create the folder if it doesn't exist
if not os.path.exists(output_folder):
    os.makedirs(output_folder)
    print(f"Created folder: {output_folder}")

# Define the full path for the output file
output_file_path = os.path.join(output_folder, output1)

# Check if the output file exists, create it if it doesn't
if not os.path.exists(output_file_path):
    with open(output_file_path, mode="w", encoding="utf-8") as f:
        f.write("")  # Initialize the file
    print(f"Created file: {output_file_path}")

# Append new output to the file
with open(output_file_path, mode="a", newline="", encoding="utf-8") as csvfile:
    writer = csv.writer(csvfile)
    # If the file is empty, add headers
    if os.path.getsize(output_file_path) == 0:
        writer.writerow(data[0].keys())  # Add headers
    for item in data:
        writer.writerow(item.values())  # Append rows

print(f"Output appended to file: {output_file_path}")

# Provide a download link for the file
files.download(output_file)

import pandas as pd
import os
import csv

# Define file paths
output_folder = "/content/drive/My Drive/LLM Response/Moiz"
existing_file_path = os.path.join(output_folder, "output1.csv")
new_output_file = "output1.csv"  # Temporary file for newly generated data

# Check if the folder exists
if not os.path.exists(output_folder):
    os.makedirs(output_folder)
    print(f"Created folder: {output_folder}")

# Combine new data with the existing file in Google Drive
if os.path.exists(existing_file_path):
    # Read the existing file
    existing_df = pd.read_csv(existing_file_path)
    # Read the new output data
    new_df = pd.read_csv(new_output_file)
    # Combine the data and remove duplicates
    combined_df = pd.concat([existing_df, new_df]).drop_duplicates()
    # Write the combined data back to the existing file
    combined_df.to_csv(existing_file_path, index=False)
    print(f"Updated file successfully saved to: {existing_file_path}")
else:
    # If the existing file does not exist, rename the new file to the target file
    os.rename(new_output_file, existing_file_path)
    print(f"Created new file at: {existing_file_path}")

# Provide a download link for the updated file
from google.colab import files
files.download(existing_file_path)
os.system("jupyter nbconvert --to notebook --execute /path/to/your/colab/file.ipynb")




